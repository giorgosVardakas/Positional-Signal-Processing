{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import Interp, Segment, patch_sampler, FeatureRep\n",
    "from seglearn.feature_functions import base_features, all_features, hudgins_features, emg_features\n",
    "from seglearn.datasets import load_watch\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from joblib import dump, load\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importand paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_path = \"./Graphs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./Data/Preprocessed_Data/data.npy\", allow_pickle=True)\n",
    "y = np.load(\"./Data/Preprocessed_Data/labels.npy\", allow_pickle=True)\n",
    "groups = np.load(\"./Data/Preprocessed_Data/groups.npy\", allow_pickle=True)\n",
    "\n",
    "# Converting them to list (needed for the learning mechanism)\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "groups = groups.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_watch()\n",
    "#X = data['X']\n",
    "#y = data['y']\n",
    "#groups = data[\"subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the features and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_strategy={115 : 300, 104 : 250, 102 : 250, 107 : 250, 101 : 250, 103 : 250, 106 : 150, 105 : 123}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = 10.\n",
    "def add_period(X):\n",
    "    \"\"\"\n",
    "    I am adding in a column to represent time (10 Hz sampling), since my data doesn't include it\n",
    "    the Interp class assumes time is the first column in the series.\n",
    "    \"\"\"\n",
    "    return np.array([np.column_stack([np.arange(len(X[i])) / sampling_frequency, X[i]]) for i in np.arange(len(X))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pype([\n",
    "    #(\"timer\", FunctionTransformer(add_period)),\n",
    "    #(\"interp\", Interp(sample_period = 1. / sampling_frequency, categorical_target=True)),\n",
    "    (\"segment\", Segment(overlap=0.2, shuffle=True)),\n",
    "    #(\"resampler\", patch_sampler(RandomUnderSampler)(sampling_strategy=\"all\")),\n",
    "    (\"features\", FeatureRep(features = {**base_features(), **emg_features()})),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA()),\n",
    "    #(\"rf\", RandomForestClassifier(criterion=\"gini\"))\n",
    "    (\"lr\",  LogisticRegression(multi_class='multinomial', max_iter=1000))\n",
    "\n",
    "], memory=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/seglearn/transform.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in np.arange(N)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV best clf score: 0.66\n",
      "Pype(steps=[('segment', Segment(overlap=0.2, shuffle=True, width=50)),\n",
      "            ('features',\n",
      "             FeatureRep(features={'abs_energy': <function abs_energy at 0x7fe1a8e77d08>,\n",
      "                                  'emg_var': <function emg_var at 0x7fe1a8e6dbf8>,\n",
      "                                  'integrated_emg': <function abs_sum at 0x7fe1a8e77c80>,\n",
      "                                  'kurt': <function kurt at 0x7fe1a8e6d1e0>,\n",
      "                                  'max': <function maximum at 0x7fe1a8e6d0d0>,\n",
      "                                  'mean': <function mean at 0x7fe...\n",
      "                                  'slope_sign_changes': slope_sign_changes(threshold=0),\n",
      "                                  'std': <function std at 0x7fe1a8e77d90>,\n",
      "                                  'var': <function var at 0x7fe1a8e77e18>,\n",
      "                                  'waveform_length': <function waveform_length at 0x7fe1a8e6d730>,\n",
      "                                  'willison_amplitude': willison_amplitude(threshold=0),\n",
      "                                  'zero_crossings': zero_crossing(threshold=0)})),\n",
      "            ('scaler', StandardScaler()), ('pca', PCA()),\n",
      "            ('lr',\n",
      "             LogisticRegression(max_iter=1000, multi_class='multinomial'))])\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupKFold(n_splits=7)\n",
    "cv = splitter.split(X, y, groups)\n",
    "\n",
    "parameters_grid = {#\"interp__sample_period\": [0.2], # 0.1\n",
    "                   \"segment__width\": [50],\n",
    "                   #\"pca__n_components\" : [60],\n",
    "                   #\"rf__n_estimators\": [50],\n",
    "                   #\"rf__min_samples_split\": [6],\n",
    "                  }\n",
    "\n",
    "\n",
    "# scoring does not work for some reason\n",
    "# it maybe always say the big category for each fragment\n",
    "# scoring=\"accuracy\", maybe because random forest has out of bag error \n",
    "grid_search = GridSearchCV(pipeline, parameters_grid, cv=cv, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV best clf score: %.2f\" % (grid_search.best_score_))\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "#scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "cv_scores = cross_validate(best_estimator, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "cv_scores_df = pd.DataFrame(cv_scores)\n",
    "cv_scores_df[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, fold, normalize=True, cmap=plt.cm.Blues):\n",
    "    title = \"Confusion matrix \" + str(fold)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize = (15, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.ylabel(\"True label\", fontsize=15)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(graphs_path + title, facecolor = \"#E0E0E0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the confusion matrix for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities = pd.read_csv(\"./Data/HomoreDataFromVariousActivities/activities.csv\")\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "for fold, split in enumerate(cv):\n",
    "    training_set, test_set = split\n",
    "    \n",
    "    # Spliting the to training and testing set\n",
    "    X_train, X_test = np.asarray(X)[training_set], np.asarray(X)[test_set]\n",
    "    y_train, y_test = np.asarray(y)[training_set], np.asarray(y)[test_set]\n",
    "    \n",
    "    visited = dict()\n",
    "    labels = list()\n",
    "    for yi_label in y_test:\n",
    "        if yi_label not in visited:\n",
    "            visited[yi_label] = True\n",
    "            labels.append(yi_label)\n",
    "\n",
    "    # Fiting the model\n",
    "    best_estimator = best_estimator.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting with trained model\n",
    "    # Transform_predict generates true and predicted target values for the segments\n",
    "    y_true, y_pred = best_estimator.transform_predict(X_test, y_test)\n",
    "    \n",
    "    # Printing accuracy score and confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    #confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    clf_accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Select the right labels and order them correcly\n",
    "    labels_names = df_activities.loc[df_activities[\"ACTIVITY_ID\"].isin(labels)]\n",
    "    labels_names = labels_names.iloc[pd.Categorical(labels_names[\"ACTIVITY_ID\"], categories = labels, ordered=True).argsort()]\n",
    "    labels_names = labels_names[\"NAME\"].to_list()\n",
    "    plot_confusion_matrix(confusion_mat, labels_names, fold, normalize=True)\n",
    "    \n",
    "    print(\"Accuracy = %.2f\" % (clf_accuracy))\n",
    "    #print(confusion_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump and Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the model\n",
    "best_estimator = best_estimator.fit(X, y)\n",
    "model_path = './Model/classifier.joblib'\n",
    "dump(best_estimator, model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "clf = load(model_path)\n",
    "dummy_data = np.random.rand(150, 6)\n",
    "dummy_data = [dummy_data]\n",
    "pred = clf.predict(dummy_data)\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
