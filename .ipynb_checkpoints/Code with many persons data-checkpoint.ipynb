{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import Interp, Segment, patch_sampler, FeatureRep\n",
    "#from seglearn.feature_functions import mean, var, std, skew, kurt\n",
    "from seglearn.feature_functions import base_features, all_features, hudgins_features, emg_features\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from joblib import dump, load\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_data_path = \"./Data/Fanis_Balaskas/accel_gyro.csv\"\n",
    "GV_data_path = \"./Data/George_Vardakas/accel_gyro_Geo.csv\"\n",
    "ΤT_data_path = \"./Data/George_Vardakas/accel_gyro_Ted.csv\"\n",
    "GP_data_path = \"./Data/George_Vardakas/accel_gyro_Gregory.csv\"\n",
    "BRO_data_path = \"./Data/George_Vardakas/accel_gyro_Bro.csv\"\n",
    "VD_data_path = \"./Data/Vagelis_Dimoulis/accel_gyro.csv\"\n",
    "\n",
    "\n",
    "df_FB_accel_gyro = pd.read_csv(FB_data_path)\n",
    "df_GV_accel_gyro = pd.read_csv(GV_data_path)\n",
    "df_TT_accel_gyro = pd.read_csv(ΤT_data_path)\n",
    "df_GP_accel_gyro = pd.read_csv(GP_data_path)\n",
    "df_BRO_accel_gyro = pd.read_csv(BRO_data_path)\n",
    "df_VD_accel_gyro = pd.read_csv(VD_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Den exw ta ms gia na spasw to shma se kathgories\n",
    "datetime_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "df_FB_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_FB_accel_gyro[\"TIMESTAMP\"])#, format=datetime_format)\n",
    "df_GV_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_GV_accel_gyro[\"TIMESTAMP\"])#, format=datetime_format)\n",
    "df_TT_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_TT_accel_gyro[\"TIMESTAMP\"])\n",
    "df_GP_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_GP_accel_gyro[\"TIMESTAMP\"])\n",
    "df_BRO_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_BRO_accel_gyro[\"TIMESTAMP\"])\n",
    "df_VD_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_VD_accel_gyro[\"TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find when activity changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling frequency of accelerometer and gyroscope (10 Hz)\n",
    "sampling_frequency = df_FB_accel_gyro.loc[1, \"TIMESTAMP\"] - df_FB_accel_gyro.loc[0, \"TIMESTAMP\"]\n",
    "\n",
    "# Finding the indexs where the samples differ more than sampling frequency\n",
    "df_FB_activity_cutoff = df_FB_accel_gyro.loc[df_FB_accel_gyro[\"TIMESTAMP\"] - df_FB_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_GV_activity_cutoff = df_GV_accel_gyro.loc[df_GV_accel_gyro[\"TIMESTAMP\"] - df_GV_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_TT_activity_cutoff = df_TT_accel_gyro.loc[df_TT_accel_gyro[\"TIMESTAMP\"] - df_TT_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_GP_activity_cutoff = df_GP_accel_gyro.loc[df_GP_accel_gyro[\"TIMESTAMP\"] - df_GP_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_BRO_activity_cutoff = df_BRO_accel_gyro.loc[df_BRO_accel_gyro[\"TIMESTAMP\"] - df_BRO_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_VD_activity_cutoff = df_VD_accel_gyro.loc[df_VD_accel_gyro[\"TIMESTAMP\"] - df_VD_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_GV_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_GV_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_FB_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_FB_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_VD_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_VD_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_TT_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_TT_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_GP_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_GP_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_BRO_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_BRO_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one dataframe for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FB_accel_gyro[\"USER_ID\"] = 0\n",
    "df_GV_accel_gyro[\"USER_ID\"] = 1\n",
    "df_VD_accel_gyro[\"USER_ID\"] = 2\n",
    "df_TT_accel_gyro[\"USER_ID\"] = 3\n",
    "df_GP_accel_gyro[\"USER_ID\"] = 4\n",
    "df_BRO_accel_gyro[\"USER_ID\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel_gyro = df_FB_accel_gyro.append(df_GV_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_VD_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_TT_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_GP_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_BRO_accel_gyro)\n",
    "df_accel_gyro.reset_index(inplace=True)\n",
    "del df_FB_accel_gyro, df_GV_accel_gyro, df_VD_accel_gyro, df_TT_accel_gyro, df_GP_accel_gyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = df_accel_gyro.loc[1, \"TIMESTAMP\"] - df_accel_gyro.loc[0, \"TIMESTAMP\"]\n",
    "# Finding the indexs where the samples differ more than sampling frequency\n",
    "activities = df_accel_gyro.loc[df_accel_gyro['TIMESTAMP'] - df_accel_gyro['TIMESTAMP'].shift() > sampling_frequency]\n",
    "# To include the last activity to the end\n",
    "activities = activities.append(df_accel_gyro.iloc[-1])\n",
    "\n",
    "# Constracting the data exactly how the seglearn needs it\n",
    "# Data must be list(np.arrays)\n",
    "# list() -> single multivariate time series\n",
    "# X[0].shape -> (n_samples, n_variables)\n",
    "# n_samples is how many data points we have in time we have\n",
    "# n_variables is how many sensors we have\n",
    "\n",
    "# I do not include the moment where activity changes\n",
    "X = list()\n",
    "y = list()\n",
    "groups = list()\n",
    "low_index = 0\n",
    "\n",
    "# Use this to throw some samples or not?\n",
    "samples_to_throw = 10\n",
    "\n",
    "for i, high_index in enumerate(activities.index):\n",
    "    low_index += samples_to_throw\n",
    "    high_index -= samples_to_throw\n",
    "    data = df_accel_gyro[[\"ACCEL_X\",\"ACCEL_Y\", \"ACCEL_Z\", \"GYRO_X\", \"GYRO_Y\", \"GYRO_Z\"]].iloc[low_index : high_index]\n",
    "    data = data.to_numpy()\n",
    "    labels = df_accel_gyro[\"ACTIVITY_ID\"].iloc[low_index]\n",
    "    user_id = df_accel_gyro[\"USER_ID\"].iloc[low_index]\n",
    "    \n",
    "    X.append(data)\n",
    "    y.append(labels)\n",
    "    groups.append(user_id)\n",
    "    \n",
    "    #print(low_index, mid_index, high_index)\n",
    "    low_index = high_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_accel_gyro[[\"ACCEL_X\",\"ACCEL_Y\", \"ACCEL_Z\", \"GYRO_X\", \"GYRO_Y\", \"GYRO_Z\", \"ACTIVITY_ID\", \"USER_ID\"]].plot(figsize=(15,8))\n",
    "#ax.vlines(activities.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_accel_gyro.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the features and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poly(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2):\n",
    "        self.poly_features = PolynomialFeatures(degree)\n",
    "    \n",
    "    def fit(self, Xt, y):\n",
    "        Xt_new = [self.poly_features.fit_transform(timeseries) for timeseries in Xt[0]]\n",
    "        Xt_new = np.array(Xt_new)\n",
    "        #self.Xt = [Xt_new, Xt[1], Xt[2]]\n",
    "        self.Xt = [Xt_new, Xt[1]]\n",
    "        return self\n",
    "            \n",
    "    def transform(self, Xt, y):\n",
    "        return self.Xt\n",
    "    \n",
    "    def fit_transform(self, Xt, y):\n",
    "        self.fit(Xt, y)\n",
    "        return self.transform(self, Xt, y)\n",
    "\n",
    "def poly_features(Xt):\n",
    "    poly_features = PolynomialFeatures(2)\n",
    "    #return poly_features.fit_transform(X)\n",
    "    \n",
    "    Xt_new = [poly_features.fit_transform(timeseries) for timeseries in Xt[0]]\n",
    "    Xt_new = np.array(Xt_new)\n",
    "    Xt = [Xt_new, Xt[1], Xt[2]]\n",
    "   \n",
    "    return Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seg = Segment(width=100, overlap=0.2)\n",
    "#d = seg.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(d), len(d[0]), len(d[1]), d[0].shape, d[0][0].shape, d[1].shape, type(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = FunctionTransformer(poly_features).fit_transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(d), len(d[0]), len(d[1]), d[0].shape, d[0][0].shape, d[1].shape, type(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = FeatureRep(features = {**base_features(), **emg_features()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = f.fit_transform(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel_gyro[\"ACTIVITY_ID\"].value_counts()\n",
    "sampling_strategy={115 : 300, 104 : 250,102 : 250,107 : 250,101 : 250,103 : 250,106 : 150,105 : 123}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = 10.\n",
    "def add_period(X):\n",
    "    \"\"\"\n",
    "    I am adding in a column to represent time (10 Hz sampling), since my data doesn't include it\n",
    "    the Interp class assumes time is the first column in the series.\n",
    "    \"\"\"\n",
    "    return np.array([np.column_stack([np.arange(len(X[i])) / sampling_frequency, X[i]]) for i in np.arange(len(X))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pype([\n",
    "    (\"timer\", FunctionTransformer(add_period)),\n",
    "    (\"interp\", Interp(sample_period = 1. / sampling_frequency, categorical_target=True)),\n",
    "    (\"segment\", Segment(overlap=0.2, shuffle=True)),\n",
    "    #(\"resampler\", patch_sampler(RandomUnderSampler)(sampling_strategy=\"all\")),\n",
    "    \n",
    "    #(\"poly_features\", FunctionTransformer(poly_features)),\n",
    "    #(\"poly_features\", poly()),\n",
    "    \n",
    "    (\"features\", FeatureRep(features = {**base_features(), **emg_features()})),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA()),\n",
    "    (\"rf\", RandomForestClassifier(criterion=\"gini\"))\n",
    "], memory=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "<ipython-input-25-2dfe244e3a25>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([np.column_stack([np.arange(len(X[i])) / sampling_frequency, X[i]]) for i in np.arange(len(X))])\n",
      "/home/geo/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6529925275621812\n",
      "Pype(steps=[('timer',\n",
      "             FunctionTransformer(func=<function add_period at 0x7ff88a2df040>)),\n",
      "            ('interp', Interp(categorical_target=True, sample_period=0.2)),\n",
      "            ('segment', Segment(overlap=0.2, shuffle=True, width=50)),\n",
      "            ('features',\n",
      "             FeatureRep(features={'abs_energy': <function abs_energy at 0x7ff894602430>,\n",
      "                                  'emg_var': <function emg_var at 0x7ff89459d3a0>,\n",
      "                                  'integrated_emg': <func...\n",
      "                                  'std': <function std at 0x7ff8946024c0>,\n",
      "                                  'var': <function var at 0x7ff894602550>,\n",
      "                                  'waveform_length': <function waveform_length at 0x7ff894602e50>,\n",
      "                                  'willison_amplitude': willison_amplitude(threshold=0),\n",
      "                                  'zero_crossings': zero_crossing(threshold=0)})),\n",
      "            ('scaler', StandardScaler()), ('pca', PCA(n_components=40)),\n",
      "            ('rf',\n",
      "             RandomForestClassifier(min_samples_split=7, n_estimators=20))])\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupKFold(n_splits=6)\n",
    "cv = splitter.split(X, y, groups)\n",
    "\n",
    "parameters_grid = {\"interp__sample_period\": [0.2], # 0.1\n",
    "                   \"segment__width\": [50],\n",
    "                   \"pca__n_components\" : [40],\n",
    "                   \"rf__n_estimators\": [20],\n",
    "                   \"rf__min_samples_split\": [7]\n",
    "                  }\n",
    "\n",
    "# scoring does not work for some reason\n",
    "# it maybe always say the big category for each fragment\n",
    "# scoring=\"accuracy\", maybe because random forest has out of bag error \n",
    "grid_search = GridSearchCV(pipeline, parameters_grid, cv=cv, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.40751958, 1.3965385 , 1.33471346, 1.62211633, 1.53879857,\n",
       "        1.51489806]),\n",
       " 'score_time': array([0.15877414, 0.17399526, 0.21740198, 0.02741313, 0.03633547,\n",
       "        0.07042098]),\n",
       " 'test_score': array([0.73255814, 0.48365019, 0.6552795 , 0.55731225, 0.75409836,\n",
       "        0.5392562 ])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "\n",
    "results = cross_validate(best_estimator, X, y, cv=cv, n_jobs=-1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, fold, normalize=True, cmap=plt.cm.Blues):\n",
    "    title = \"Confusion matrix \" + str(fold)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize = (15, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.ylabel(\"True label\", fontsize=15)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(graphs_path + title, facecolor = \"#E0E0E0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the confusion matrix for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/geo/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-2dfe244e3a25>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([np.column_stack([np.arange(len(X[i])) / sampling_frequency, X[i]]) for i in np.arange(len(X))])\n",
      "/home/geo/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n",
      "<ipython-input-25-2dfe244e3a25>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([np.column_stack([np.arange(len(X[i])) / sampling_frequency, X[i]]) for i in np.arange(len(X))])\n",
      "/home/geo/.local/lib/python3.8/site-packages/seglearn/transform.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  Xt = np.array([sliding_tensor(Xt[i], self.width, self._step, self.order)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2250, 1118]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-137657e2467a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Printing accuracy score and confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mconfusion_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mclf_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2250, 1118]"
     ]
    }
   ],
   "source": [
    "# This pipe is only for data transformations\n",
    "transform_pipe = Pype([\n",
    "    (\"segment\", best_estimator.named_steps[\"segment\"]),\n",
    "    (\"features\", best_estimator.named_steps[\"features\"]),\n",
    "    (\"scaler\", best_estimator.named_steps[\"scaler\"]),\n",
    "    (\"pca\", best_estimator.named_steps[\"pca\"]),\n",
    "], memory=None)\n",
    "\n",
    "df_activities = pd.read_csv(\"./Data/HomoreDataFromVariousActivities/activities.csv\")\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "for fold, split in enumerate(cv):\n",
    "    training_set, test_set = split\n",
    "    \n",
    "    # Spliting the to training and testing set\n",
    "    x_train, x_test = np.asarray(X)[training_set], np.asarray(X)[test_set]\n",
    "    y_train, y_test = np.asarray(y)[training_set], np.asarray(y)[test_set]\n",
    "    \n",
    "    visited = dict()\n",
    "    labels = list()\n",
    "    for yi_label in y_test:\n",
    "        if yi_label not in visited:\n",
    "            visited[yi_label] = True\n",
    "            labels.append(yi_label)\n",
    "    \n",
    "    print(x_test[0].shape)\n",
    "    # Transforming the test data to fit the model predictions\n",
    "    _, y_test_trans = transform_pipe.fit_transform(x_test, y_test)\n",
    "    \n",
    "    # Fiting the model\n",
    "    best_estimator.fit(x_train, y_train)\n",
    "    \n",
    "    # Predicting with trained model\n",
    "    y_pred = best_estimator.predict(x_test)\n",
    "    \n",
    "    # Printing accuracy score and confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test_trans, y_pred, labels=labels)\n",
    "    clf_accuracy = accuracy_score(y_test_trans, y_pred)\n",
    "    \n",
    "    # Select the right labels and order them correcly\n",
    "    labels_names = df_activities.loc[df_activities[\"ACTIVITY_ID\"].isin(labels)]\n",
    "    labels_names = labels_names.iloc[pd.Categorical(labels_names[\"ACTIVITY_ID\"], categories = labels, ordered=True).argsort()]\n",
    "    labels_names = labels_names[\"NAME\"].to_list()\n",
    "    plot_confusion_matrix(confusion_mat, labels_names, fold, normalize=True)\n",
    "    \n",
    "    print(clf_accuracy)\n",
    "    print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump and Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = best_estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Model/classifier.joblib'\n",
    "dump(best_estimator, model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = np.random.rand(150, 6)\n",
    "dummy_data = [dummy_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel_gyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_accel_gyro[[\"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\", \"GYRO_X\", \"GYRO_Y\", \"GYRO_Z\"]].iloc[460799 - 1000:460799]\n",
    "data = [data.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
