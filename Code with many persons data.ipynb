{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import FeatureRep, Segment\n",
    "#from seglearn.feature_functions import mean, var, std, skew, kurt\n",
    "from seglearn.feature_functions import base_features, all_features, hudgins_features, emg_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from joblib import dump, load\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_data_path = \"./Data/Fanis_Balaskas/accel_gyro.csv\"\n",
    "GV_data_path = \"./Data/George_Vardakas/accel_gyro_Geo.csv\"\n",
    "ΤT_data_path = \"./Data/George_Vardakas/accel_gyro_Ted.csv\"\n",
    "GP_data_path = \"./Data/George_Vardakas/accel_gyro_Gregory.csv\"\n",
    "BRO_data_path = \"./Data/George_Vardakas/accel_gyro_Bro.csv\"\n",
    "VD_data_path = \"./Data/Vagelis_Dimoulis/accel_gyro.csv\"\n",
    "\n",
    "\n",
    "df_FB_accel_gyro = pd.read_csv(FB_data_path)\n",
    "df_GV_accel_gyro = pd.read_csv(GV_data_path)\n",
    "df_TT_accel_gyro = pd.read_csv(ΤT_data_path)\n",
    "df_GP_accel_gyro = pd.read_csv(GP_data_path)\n",
    "df_BRO_accel_gyro = pd.read_csv(BRO_data_path)\n",
    "df_VD_accel_gyro = pd.read_csv(VD_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Den exw ta ms gia na spasw to shma se kathgories\n",
    "datetime_format = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "df_FB_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_FB_accel_gyro[\"TIMESTAMP\"])#, format=datetime_format)\n",
    "df_GV_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_GV_accel_gyro[\"TIMESTAMP\"])#, format=datetime_format)\n",
    "df_TT_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_TT_accel_gyro[\"TIMESTAMP\"])\n",
    "df_GP_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_GP_accel_gyro[\"TIMESTAMP\"])\n",
    "df_BRO_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_BRO_accel_gyro[\"TIMESTAMP\"])\n",
    "df_VD_accel_gyro[\"TIMESTAMP\"] = pd.to_datetime(df_VD_accel_gyro[\"TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find when activity changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling frequency of accelerometer and gyroscope (10 Hz)\n",
    "sampling_frequency = df_FB_accel_gyro.loc[1, \"TIMESTAMP\"] - df_FB_accel_gyro.loc[0, \"TIMESTAMP\"]\n",
    "\n",
    "# Finding the indexs where the samples differ more than sampling frequency\n",
    "df_FB_activity_cutoff = df_FB_accel_gyro.loc[df_FB_accel_gyro[\"TIMESTAMP\"] - df_FB_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_GV_activity_cutoff = df_GV_accel_gyro.loc[df_GV_accel_gyro[\"TIMESTAMP\"] - df_GV_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_TT_activity_cutoff = df_TT_accel_gyro.loc[df_TT_accel_gyro[\"TIMESTAMP\"] - df_TT_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_GP_activity_cutoff = df_GP_accel_gyro.loc[df_GP_accel_gyro[\"TIMESTAMP\"] - df_GP_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_BRO_activity_cutoff = df_BRO_accel_gyro.loc[df_BRO_accel_gyro[\"TIMESTAMP\"] - df_BRO_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]\n",
    "df_VD_activity_cutoff = df_VD_accel_gyro.loc[df_VD_accel_gyro[\"TIMESTAMP\"] - df_VD_accel_gyro[\"TIMESTAMP\"].shift() > sampling_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_GV_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_GV_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_FB_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_FB_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_VD_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_VD_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_TT_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_TT_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_GP_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_GP_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_BRO_accel_gyro[[\"ACTIVITY_ID\", \"ACCEL_X\", \"ACCEL_Y\", \"ACCEL_Z\"]].plot(figsize = (15, 8))\n",
    "#ax.vlines(df_BRO_activity_cutoff.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one dataframe for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FB_accel_gyro[\"USER_ID\"] = 0\n",
    "df_GV_accel_gyro[\"USER_ID\"] = 1\n",
    "df_VD_accel_gyro[\"USER_ID\"] = 2\n",
    "df_TT_accel_gyro[\"USER_ID\"] = 3\n",
    "df_GP_accel_gyro[\"USER_ID\"] = 4\n",
    "df_BRO_accel_gyro[\"USER_ID\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accel_gyro = df_FB_accel_gyro.append(df_GV_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_VD_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_TT_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_GP_accel_gyro)\n",
    "df_accel_gyro = df_accel_gyro.append(df_BRO_accel_gyro)\n",
    "df_accel_gyro.reset_index(inplace=True)\n",
    "del df_FB_accel_gyro, df_GV_accel_gyro, df_VD_accel_gyro, df_TT_accel_gyro, df_GP_accel_gyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = df_accel_gyro.loc[1, \"TIMESTAMP\"] - df_accel_gyro.loc[0, \"TIMESTAMP\"]\n",
    "# Finding the indexs where the samples differ more than sampling frequency\n",
    "activities = df_accel_gyro.loc[df_accel_gyro['TIMESTAMP'] - df_accel_gyro['TIMESTAMP'].shift() > sampling_frequency]\n",
    "# To include the last activity to the end\n",
    "activities = activities.append(df_accel_gyro.iloc[-1])\n",
    "\n",
    "# Constracting the data exactly how the seglearn needs it\n",
    "# Data must be list(np.arrays)\n",
    "# list() -> single multivariate time series\n",
    "# X[0].shape -> (n_samples, n_variables)\n",
    "# n_samples is how many data points we have in time we have\n",
    "# n_variables is how many sensors we have\n",
    "\n",
    "# I do not include the moment where activity changes\n",
    "X = list()\n",
    "y = list()\n",
    "groups = list()\n",
    "low_index = 0\n",
    "\n",
    "# Use this to throw some samples or not?\n",
    "samples_to_throw = 10\n",
    "\n",
    "for i, high_index in enumerate(activities.index):\n",
    "    low_index += samples_to_throw\n",
    "    high_index -= samples_to_throw\n",
    "    data = df_accel_gyro[[\"ACCEL_X\",\"ACCEL_Y\", \"ACCEL_Z\", \"GYRO_X\", \"GYRO_Y\", \"GYRO_Z\"]].iloc[low_index : high_index]\n",
    "    data = data.to_numpy()\n",
    "    labels = df_accel_gyro[\"ACTIVITY_ID\"].iloc[low_index]\n",
    "    user_id = df_accel_gyro[\"USER_ID\"].iloc[low_index]\n",
    "    \n",
    "    X.append(data)\n",
    "    y.append(labels)\n",
    "    groups.append(user_id)\n",
    "    \n",
    "    #print(low_index, mid_index, high_index)\n",
    "    low_index = high_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_accel_gyro[[\"ACCEL_X\",\"ACCEL_Y\", \"ACCEL_Z\", \"GYRO_X\", \"GYRO_Y\", \"GYRO_Z\", \"ACTIVITY_ID\", \"USER_ID\"]].plot(figsize=(15,8))\n",
    "#ax.vlines(activities.index, ymin=-20, ymax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                               34799\n",
       "TIMESTAMP      2021-01-27 22:59:23.771000\n",
       "ACCEL_X                          -9.20718\n",
       "ACCEL_Y                          -2.10337\n",
       "ACCEL_Z                            2.3823\n",
       "GYRO_X                         -0.0138483\n",
       "GYRO_Y                                  0\n",
       "GYRO_Z                         -0.0106525\n",
       "ACTIVITY_ID                           103\n",
       "USER_ID                                 5\n",
       "Name: 460799, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accel_gyro.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the features and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pype([\n",
    "    (\"segment\", Segment()),\n",
    "    # 120 features are created\n",
    "    (\"features\", FeatureRep(features = {**base_features(), **emg_features()})),\n",
    "    #(\"features\", FeatureRep(features = base_features())),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA()),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "], memory=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/seglearn/transform.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in np.arange(N)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7256329737321342\n",
      "Pype(steps=[('segment', Segment(overlap=0.1, width=350)),\n",
      "            ('features',\n",
      "             FeatureRep(features={'abs_energy': <function abs_energy at 0x7fdf62992ae8>,\n",
      "                                  'emg_var': <function emg_var at 0x7fdf629949d8>,\n",
      "                                  'integrated_emg': <function abs_sum at 0x7fdf62992a60>,\n",
      "                                  'kurt': <function kurt at 0x7fdf62992f28>,\n",
      "                                  'max': <function maximum at 0x7fdf62992e18>,\n",
      "                                  'mean': <function mean at 0x7fdf629927b8>,\n",
      "                                  '...\n",
      "                                  'slope_sign_changes': slope_sign_changes(threshold=0),\n",
      "                                  'std': <function std at 0x7fdf62992b70>,\n",
      "                                  'var': <function var at 0x7fdf62992bf8>,\n",
      "                                  'waveform_length': <function waveform_length at 0x7fdf62994510>,\n",
      "                                  'willison_amplitude': willison_amplitude(threshold=0),\n",
      "                                  'zero_crossings': zero_crossing(threshold=0)})),\n",
      "            ('scaler', StandardScaler()), ('pca', PCA(n_components=90)),\n",
      "            ('rf', RandomForestClassifier(n_estimators=40))])\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupKFold(n_splits=6)\n",
    "cv = splitter.split(X, y, groups)\n",
    "\n",
    "parameters_grid = {\"segment__width\": list(range(250, 400, 50)),\n",
    "                   \"segment__overlap\": np.arange(0.1, 0.3, 0.1).tolist(),\n",
    "                   \"pca__n_components\" : [70, 80, 90], #[100, 110, 120],\n",
    "                   \"rf__n_estimators\": range(20, 45, 5)\n",
    "                  }\n",
    "\n",
    "# scoring does not work for some reason\n",
    "# it maybe always say the big category for each fragment\n",
    "# scoring=\"accuracy\", maybe because random forest has out of bag error \n",
    "grid_search = GridSearchCV(pipeline, parameters_grid, cv=cv, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.82947373, 0.81262946, 0.81891632, 0.98524594, 0.97000432,\n",
       "        0.90938759]),\n",
       " 'score_time': array([0.12385249, 0.12554502, 0.14289331, 0.03072405, 0.03742695,\n",
       "        0.06811762]),\n",
       " 'test_score': array([0.79553903, 0.50304878, 0.81343284, 0.66071429, 0.80769231,\n",
       "        0.69709544])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "\n",
    "results = cross_validate(best_estimator, X, y, cv=cv, n_jobs=-1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, fold, normalize=True, cmap=plt.cm.Blues):\n",
    "    title = \"Confusion matrix \" + str(fold)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize = (15, 8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.ylabel(\"True label\", fontsize=15)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(graphs_path + title, facecolor = \"#E0E0E0\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the confusion matrix for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass memory=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/home/geo/.local/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/home/geo/.local/lib/python3.6/site-packages/seglearn/transform.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in np.arange(N)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65,)\n",
      "(590, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geo/.local/lib/python3.6/site-packages/seglearn/transform.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in np.arange(N)])\n",
      "/home/geo/.local/lib/python3.6/site-packages/seglearn/transform.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  for i in np.arange(N)])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "plot_confusion_matrix only supports classifiers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-788dddc6acb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlabels_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ACTIVITY_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlabels_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NAME\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plot_confusion_matrix only supports classifiers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: plot_confusion_matrix only supports classifiers"
     ]
    }
   ],
   "source": [
    "# This pipe is only for data transformations\n",
    "transform_pipe = Pype([\n",
    "    (\"segment\", best_estimator.named_steps[\"segment\"]),\n",
    "    (\"features\", best_estimator.named_steps[\"features\"]),\n",
    "    (\"scaler\", best_estimator.named_steps[\"scaler\"]),\n",
    "    (\"pca\", best_estimator.named_steps[\"pca\"]),\n",
    "], memory=None)\n",
    "\n",
    "df_activities = pd.read_csv(\"./Data/HomoreDataFromVariousActivities/activities.csv\")\n",
    "\n",
    "cv = splitter.split(X, y, groups)\n",
    "for fold, split in enumerate(cv):\n",
    "    training_set, test_set = split\n",
    "    \n",
    "    # Spliting the to training and testing set\n",
    "    x_train, x_test = np.asarray(X)[training_set], np.asarray(X)[test_set]\n",
    "    y_train, y_test = np.asarray(y)[training_set], np.asarray(y)[test_set]\n",
    "    \n",
    "    visited = dict()\n",
    "    labels = list()\n",
    "    for yi_label in y_test:\n",
    "        if yi_label not in visited:\n",
    "            visited[yi_label] = True\n",
    "            labels.append(yi_label)\n",
    "    \n",
    "    print(x_test[0].shape)\n",
    "    # Transforming the test data to fit the model predictions\n",
    "    _, y_test_trans = transform_pipe.fit_transform(x_test, y_test)\n",
    "    \n",
    "    # Fiting the model\n",
    "    best_estimator.fit(x_train, y_train)\n",
    "    \n",
    "    # Predicting with trained model\n",
    "    y_pred = best_estimator.predict(x_test)\n",
    "    \n",
    "    # Printing accuracy score and confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_test_trans, y_pred, labels=labels)\n",
    "    clf_accuracy = accuracy_score(y_test_trans, y_pred)\n",
    "    \n",
    "    # Select the right labels and order them correcly\n",
    "    labels_names = df_activities.loc[df_activities[\"ACTIVITY_ID\"].isin(labels)]\n",
    "    labels_names = labels_names.iloc[pd.Categorical(labels_names[\"ACTIVITY_ID\"], categories = labels, ordered=True).argsort()]\n",
    "    labels_names = labels_names[\"NAME\"].to_list()\n",
    "    plot_confusion_matrix(confusion_mat, labels_names, fold, normalize=True)\n",
    "    \n",
    "    print(clf_accuracy)\n",
    "    print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump and Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './Model/classifier.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Model/classifier.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(best_estimator, model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = np.random.rand(350, 6)\n",
    "dummy_data = [dummy_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
